{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "name": "Week_4_Lab_Neural_Networks_in_Practice.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sasi6996/EAI6000/blob/master/Week_4_Lab_Neural_Networks_in_Practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jE7Q3E9i9Evd"
      },
      "source": [
        "# Week 4 Lab: Neural Networks in practice"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "hide_input": false,
        "id": "gWDjQyV59Evg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "18952fc7-6434-4c51-9da0-1778c05fa829"
      },
      "source": [
        "# Global imports and settings\n",
        "from IPython.display import set_matplotlib_formats, display\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from cycler import cycler\n",
        "import keras\n",
        "print(\"Using Keras\",keras.__version__)\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.dpi'] = 125 # Use 300 for PDF, 100 for slides\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using Keras 2.2.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_style": "center",
        "colab_type": "text",
        "id": "QJdIHypq9Evk"
      },
      "source": [
        "### Overview\n",
        "* Solving basic classification and regression problems\n",
        "* Handling textual data\n",
        "* Model selection (and overfitting)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "njT6MasM9Evl"
      },
      "source": [
        "## Solving basic problems\n",
        "* Binary classification (of movie reviews)\n",
        "* Multiclass classification (of news topics)\n",
        "* Regression (of house prices)\n",
        "\n",
        "Examples from _Deep Learning with Python_, by _Fran√ßois Chollet_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xsUfzIi59Evn"
      },
      "source": [
        "### Binary classification\n",
        "* Dataset: 50,000 IMDB reviews, labeled positive (1) or negative (0)\n",
        "    - Included in Keras, with a 50/50 train-test split\n",
        "* Each row is one review, with only the 10,000 most frequent words retained\n",
        "* Each word is replaced by a _word index_ (word ID)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "hide_input": true,
        "id": "Mk42XDWZ9Evn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "c2e2fe4a-4b14-4fdc-88fc-1366817014b0"
      },
      "source": [
        "from keras.datasets import imdb\n",
        "# Download IMDB data with 10000 most frequent words\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)\n",
        "print(\"Encoded review: \", train_data[0][0:10])\n",
        "\n",
        "word_index = imdb.get_word_index()\n",
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
        "print(\"Original review: \", ' '.join([reverse_word_index.get(i - 3, '?') for i in train_data[0]][0:10]))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 3s 0us/step\n",
            "Encoded review:  [1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65]\n",
            "Downloading data from https://s3.amazonaws.com/text-datasets/imdb_word_index.json\n",
            "1646592/1641221 [==============================] - 1s 1us/step\n",
            "Original review:  ? this film was just brilliant casting location scenery story\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gbHSVzma9Evq"
      },
      "source": [
        "#### Preprocessing\n",
        "* We can't input lists of categorical value to a neural net, we need to create tensors\n",
        "* One-hot-encoding:\n",
        "    -  10000 features, '1.0' if the word occurs\n",
        "* Word embeddings (word2vec):\n",
        "    - Map each word to a dense vector that represents it (it's _embedding_)\n",
        "    - _Embedding_ layer: pre-trained layer that looks up the embedding in a dictionary \n",
        "    - Converts 2D tensor of word indices (zero-padded) to 3D tensor of embeddings\n",
        "* Let's do One-Hot-Encoding for now. We'll come back to _Embedding_ layers.\n",
        "* Also vectorize the labels: from 0/1 to float\n",
        "    - Binary classification works with one output node"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "hide_input": true,
        "id": "nSzfO-XG9Evs",
        "colab": {}
      },
      "source": [
        "# Custom implementation of one-hot-encoding\n",
        "def vectorize_sequences(sequences, dimension=10000):\n",
        "    results = np.zeros((len(sequences), dimension))\n",
        "    for i, sequence in enumerate(sequences):\n",
        "        results[i, sequence] = 1.  # set specific indices of results[i] to 1s\n",
        "    return results\n",
        "x_train = vectorize_sequences(train_data)\n",
        "x_test = vectorize_sequences(test_data)\n",
        "y_train = np.asarray(train_labels).astype('float32')\n",
        "y_test = np.asarray(test_labels).astype('float32')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4_JHofjQRme1"
      },
      "source": [
        "#### Understanding the format of IMDB dataset\n",
        "1. Train_data and test_data are an array of lists. What does the length of this array correspond to? What does the length of each list correspond to?\n",
        "2. What are the sizes of the vectorized x_train and x_test? What do the dimensions correspond to?\n",
        "3. What is the most common word in the first review in the training data? Hint: use the word index (see above)? \n",
        "4. Print the first review to verify. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ceBrbGWxRLJK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "18092ae3-4a4c-4eb4-d7d5-b72e555d0e8c"
      },
      "source": [
        "train_length = len(x_train)\n",
        "test_length = len(x_test)\n",
        "\n",
        "print(train_length)\n",
        "print(test_length)\n",
        "\n",
        "display(x_train.shape)\n",
        "display(x_test.shape)\n",
        "\n",
        "n = len(word_index)\n",
        "print(\"Number distinct words in reviews = %d\" % n)\n",
        "\n",
        "words_index = []\n",
        "for (k,v) in word_index.items():\n",
        "  words_index.append((k,v))\n",
        "\n",
        "sorted_list = sorted(words_index, key=lambda x: x[1])\n",
        "\n",
        "print(\"The most commonly used word:\")\n",
        "print(sorted_list[0:1])\n",
        "\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25000\n",
            "25000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(25000, 10000)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(25000, 10000)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Number distinct words in reviews = 88584\n",
            "The most commonly used word:\n",
            "[('the', 1)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GVar7SNn9Evu"
      },
      "source": [
        "#### Building the network\n",
        "* We can solve this problem using a network of _Dense_ layers and the _ReLU_ activation function.\n",
        "* How many layers? How many hidden units for layer?\n",
        "    - Start with 2 layers of 16 hidden units each\n",
        "    - We'll optimize this soon\n",
        "* Output layer: single unit with _sigmoid_ activation function\n",
        "    - Close to 1: positive review, close to 0: negative review\n",
        "* Use binary_crossentropy loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wglvzu9E9Evx",
        "colab": {}
      },
      "source": [
        "from keras import models\n",
        "from keras import layers \n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
        "model.add(layers.Dense(16, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0S_Fnh169Ev0"
      },
      "source": [
        "#### Model selection\n",
        "* How many epochs do we need for training?\n",
        "* Take a validation set of 10,000 samples from the training set\n",
        "* Train the neural net and track the loss after every iteration on the validation set\n",
        "    - This is returned as a `History` object by the `fit()` function \n",
        "* We start with 20 epochs in minibatches of 512 samples\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "b3aTf8rL9Ev0",
        "colab": {}
      },
      "source": [
        "x_val, partial_x_train = x_train[:10000], x_train[10000:]\n",
        "y_val, partial_y_train = y_train[:10000], y_train[10000:] \n",
        "history = model.fit(partial_x_train, partial_y_train,\n",
        "                    epochs=20, batch_size=512, verbose=2,\n",
        "                    validation_data=(x_val, y_val))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PrxYLkMX9Ev3"
      },
      "source": [
        "#### Evaluate model performance during training\n",
        "1. Plot the training and validation loss as a function of training epoch. Describe what happens during the training in terms of under or overfitting.\n",
        "2. Plot the training and validation accuracy as a function of the training epoch.\n",
        "\n",
        "Hint: these quantities are contained in the dict history.history."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "hide_input": true,
        "id": "4YH3yVZy9Ev4",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "hide_input": true,
        "id": "wrJhYEe09Ev-",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FWDq34yW9EwA"
      },
      "source": [
        "#### Early stopping\n",
        "One simple technique to avoid overfitting is to use the validation set to 'tune' the optimal number of epochs\n",
        "* In this case, we could stop after 4 epochs\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "colab_type": "code",
        "id": "Csk3_Yn09EwB",
        "colab": {}
      },
      "source": [
        "#@title\n",
        "model.fit(x_train, y_train, epochs=4, batch_size=512, verbose=2)\n",
        "result = model.evaluate(x_test, y_test)\n",
        "print(\"Loss: {:.4f}, Accuracy:  {:.4f}\".format(*result))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5Mo4BX1N9EwD"
      },
      "source": [
        "#### Predictions\n",
        "1. Print the first review that were correctly classified along with the predicted value.\n",
        "2. Print the first review that were misclassified along with the predicted value. Can you explain why the model likely failed? How confident was the model?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fZB5xKAh9EwE",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "W9_hvA-lanwt"
      },
      "source": [
        "#### Takeaways\n",
        "* Neural nets require a lot of preprocessing to create tensors\n",
        "* Dense layers with ReLU activation can solve a wide range of problems\n",
        "* Binary classification can be done with a Dense layer with a single unit, sigmoid activation, and binary cross-entropy loss\n",
        "* Neural nets overfit easily\n",
        "* Many design choices have an effect on accuracy and overfitting. One can try:\n",
        "    - 1 or 3 hidden layers\n",
        "    - more or fewer hidden units (e.g. 64)\n",
        "    - MSE loss instead of binary cross-entropy\n",
        "    - `tanh` activation instead of `ReLU`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WchP4Tgu9ExF"
      },
      "source": [
        "### Regularization: build smaller networks\n",
        "* The easiest way to avoid overfitting is to use a simpler model\n",
        "* The number of learnable parameters is called the model _capacity_\n",
        "* A model with more parameters has a higher _memorization capacity_\n",
        "    - The entire training set can be `stored` in the weights\n",
        "    - Learns the mapping from training examples to outputs\n",
        "* Forcing the model to be small forces it to learn a compressed representation that generalizes better\n",
        "    - Always a trade-off between too much and too little capacity\n",
        "* Start with few layers and parameters, incease until you see diminisching returns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wo8UGPGQ9ExG"
      },
      "source": [
        "Let's try this on our movie review data, with 4 units per layer\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rFjmZtyl9ExG",
        "colab": {}
      },
      "source": [
        "from keras.datasets import imdb\n",
        "import numpy as np\n",
        "\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)\n",
        "\n",
        "def vectorize_sequences(sequences, dimension=10000):\n",
        "    # Create an all-zero matrix of shape (len(sequences), dimension)\n",
        "    results = np.zeros((len(sequences), dimension))\n",
        "    for i, sequence in enumerate(sequences):\n",
        "        results[i, sequence] = 1.  # set specific indices of results[i] to 1s\n",
        "    return results\n",
        "\n",
        "# Our vectorized training data\n",
        "x_train = vectorize_sequences(train_data)\n",
        "# Our vectorized test data\n",
        "x_test = vectorize_sequences(test_data)\n",
        "# Our vectorized labels\n",
        "y_train = np.asarray(train_labels).astype('float32')\n",
        "y_test = np.asarray(test_labels).astype('float32')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0RYsX-RF9ExJ",
        "colab": {}
      },
      "source": [
        "from keras import models\n",
        "from keras import layers \n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "original_model = models.Sequential()\n",
        "original_model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
        "original_model.add(layers.Dense(16, activation='relu'))\n",
        "original_model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "original_model.compile(optimizer='rmsprop',\n",
        "                       loss='binary_crossentropy',\n",
        "                       metrics=['acc'])\n",
        "\n",
        "smaller_model = models.Sequential()\n",
        "smaller_model.add(layers.Dense(4, activation='relu', input_shape=(10000,)))\n",
        "smaller_model.add(layers.Dense(4, activation='relu'))\n",
        "smaller_model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "smaller_model.compile(optimizer='rmsprop',\n",
        "                      loss='binary_crossentropy',\n",
        "                      metrics=['acc'])\n",
        "original_hist = original_model.fit(x_train, y_train,\n",
        "                                   epochs=20,\n",
        "                                   batch_size=512, verbose=2,\n",
        "                                   validation_data=(x_test, y_test))\n",
        "smaller_model_hist = smaller_model.fit(x_train, y_train,\n",
        "                                       epochs=20,\n",
        "                                       batch_size=512, verbose=2,\n",
        "                                       validation_data=(x_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rCV6TJ899ExL"
      },
      "source": [
        "1. Plot the validation loss for the original and smaller models. How does the smaller model behave compared to the original?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "m7kNvhh-9ExN",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_u-YXpCZ9ExP"
      },
      "source": [
        "### Regularization: Weight regularization\n",
        "* As we did many times before, we can also add weight regularization to our loss function\n",
        "- L1 regularization: leads to _sparse networks_ with many weights that are 0\n",
        "- L2 regularization: leads to many very small weights\n",
        "    - Also called _weight decay_ in neural net literature\n",
        "* In Keras, add `kernel_regularizer` to every layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kXOqh2uU9ExR",
        "colab": {}
      },
      "source": [
        "from keras import regularizers\n",
        "from keras import models\n",
        "from keras import layers \n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "l2_model = models.Sequential()\n",
        "l2_model.add(layers.Dense(16, kernel_regularizer=regularizers.l2(0.001),\n",
        "                          activation='relu', input_shape=(10000,)))\n",
        "l2_model.add(layers.Dense(16, kernel_regularizer=regularizers.l2(0.001),\n",
        "                          activation='relu'))\n",
        "l2_model.add(layers.Dense(1, activation='sigmoid'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mzNX3bHo9ExT",
        "colab": {}
      },
      "source": [
        "l2_model.compile(optimizer='rmsprop',\n",
        "                 loss='binary_crossentropy',\n",
        "                 metrics=['acc'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2Gg8hLY69ExU",
        "colab": {}
      },
      "source": [
        "l2_model_hist = l2_model.fit(x_train, y_train,\n",
        "                             epochs=20,\n",
        "                             batch_size=512, verbose=2,\n",
        "                             validation_data=(x_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vKOw1jNlibXK"
      },
      "source": [
        "1. Plot the validation loss for the original and l2 regularized models. How does the regularized model behave compared to the original?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9qYQvkIe9ExW",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OFbL6yJF9ExZ"
      },
      "source": [
        "### Regularization: dropout\n",
        "* One of the most effective and commonly used regularization techniques\n",
        "* Randomly set a number of outputs of the layer to 0\n",
        "* Idea: break up accidental non-significant learned patterns \n",
        "* _Dropout rate_: fraction of the outputs that are zeroed-out\n",
        "    - Usually between 0.2 and 0.5\n",
        "* At test time, nothing is dropped out, but the output values are scaled down by the dropout rate\n",
        "    - Balances out that more units are active than during training\n",
        "* In Keras: add `Dropout` layers between the normal layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SUHgbTz49Exa",
        "colab": {}
      },
      "source": [
        "from keras import models\n",
        "from keras import layers \n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "dpt_model = models.Sequential()\n",
        "dpt_model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
        "dpt_model.add(layers.Dropout(0.5))\n",
        "dpt_model.add(layers.Dense(16, activation='relu'))\n",
        "dpt_model.add(layers.Dropout(0.5))\n",
        "dpt_model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "dpt_model.compile(optimizer='rmsprop',\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['acc'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2QSG_vWa9Exb",
        "colab": {}
      },
      "source": [
        "dpt_model_hist = dpt_model.fit(x_train, y_train,\n",
        "                               epochs=20,\n",
        "                               \n",
        "                               batch_size=512, verbose=2,\n",
        "                               validation_data=(x_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VZx6VNXn9Exd"
      },
      "source": [
        "1. Plot the validation loss for the original and dropout models. How does the dropout model behave compared to the original?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-ZS_z7xM9Exe",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ORAwOqdV9Exf"
      },
      "source": [
        "### Regularization recap\n",
        "* Get more training data\n",
        "* Reduce the capacity of the network\n",
        "* Add weight regularization\n",
        "* Add dropout\n",
        "* Either start with a simple model and add capacity\n",
        "* Or, start with a complex model and then regularize by adding weight regularization and dropout"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1xwfZv_E9Ewq"
      },
      "source": [
        "### Regression\n",
        "* Dataset: 506 examples of houses and sale prices (Boston)\n",
        "    - Included in Keras, with a 1/5 train-test split\n",
        "* Each row is one house price, described by numeric properties of the house and neighborhood\n",
        "* Small dataset, non-normalized features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZlUGyLJS9Ewr",
        "colab": {}
      },
      "source": [
        "from keras.datasets import boston_housing\n",
        "\n",
        "(train_data, train_targets), (test_data, test_targets) =  boston_housing.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gX4hkxBb9Ewt"
      },
      "source": [
        "#### Preprocessing\n",
        "* Neural nets work a lot better if we normalize the features first. \n",
        "* Keras has no built-in support so we have to do this manually (or with scikit-learn)\n",
        "    - Again, be careful not to look at the test data during normalization\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "d-W92M6I9Ewt",
        "colab": {}
      },
      "source": [
        "mean, std = train_data.mean(axis=0), train_data.std(axis=0)\n",
        "train_data -= mean\n",
        "train_data /= std\n",
        "\n",
        "test_data -= mean\n",
        "test_data /= std"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OwM0wfKE9Ewx"
      },
      "source": [
        "#### Building the network\n",
        "* This is a small dataset, so easy to overfit\n",
        "    * We use 2 hidden layers of 64 units each\n",
        "* Use smaller batches, more epochs\n",
        "* Since we want scalar output, the output layer is one unit without activation\n",
        "* Loss function is Mean Squared Error (bigger penalty)\n",
        "* Evaluation metric is Mean Absolute Error (more interpretable)\n",
        "* We will also use cross-validation, so we wrap the model building in a function, so that we can call it multiple times\n",
        "\n",
        "1. Create a function build_model that returns the neural network model described above"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vL_p-NcX9Ewz",
        "colab": {}
      },
      "source": [
        "def build_model():\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Dense(64, activation='relu',\n",
        "                           input_shape=(train_data.shape[1],)))\n",
        "    model.add(layers.Dense(64, activation='relu'))\n",
        "    model.add(layers.Dense(1))\n",
        "    model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "t2seh8XW9Ew1"
      },
      "source": [
        "#### Cross-validation\n",
        "* Keras does not have support for cross-validation\n",
        "* We can implement cross-validation ourselves (seeprovided code below)\n",
        "* Alternatively, we can wrap a Keras model as a scikit-learn estimator\n",
        "* Generally speaking, cross-validation is tricky with neural nets\n",
        "    * Some fold may not converge, or fluctuate on random initialization\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VupbDC1U9Ew3",
        "colab": {}
      },
      "source": [
        "# implementation of cross-validation\n",
        "import numpy as np\n",
        "\n",
        "k = 4\n",
        "num_val_samples = len(train_data) // k\n",
        "num_epochs = 20\n",
        "all_scores = []\n",
        "for i in range(k):\n",
        "    print('processing fold #', i)\n",
        "    # Prepare the validation data: data from partition # k\n",
        "    val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples]\n",
        "    val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples]\n",
        "\n",
        "    # Prepare the training data: data from all other partitions\n",
        "    partial_train_data = np.concatenate(\n",
        "        [train_data[:i * num_val_samples],\n",
        "         train_data[(i + 1) * num_val_samples:]],\n",
        "        axis=0)\n",
        "    partial_train_targets = np.concatenate(\n",
        "        [train_targets[:i * num_val_samples],\n",
        "         train_targets[(i + 1) * num_val_samples:]],\n",
        "        axis=0)\n",
        "\n",
        "    # Build the Keras model (already compiled)\n",
        "    model = build_model()\n",
        "    # Train the model (in silent mode, verbose=0)\n",
        "    model.fit(partial_train_data, partial_train_targets,\n",
        "              epochs=num_epochs, batch_size=1, verbose=0)\n",
        "    # Evaluate the model on the validation data\n",
        "    val_mse, val_mae = model.evaluate(val_data, val_targets, verbose=2)\n",
        "    all_scores.append(val_mae)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OcEyK-E59Ew8"
      },
      "source": [
        "1. Train for longer (200 epochs) and keep track of loss after every epoch. Plot and describe the loss as a function of epoch number."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tXsVlXqw9Ew8",
        "colab": {}
      },
      "source": [
        "from keras import backend as K\n",
        "K.clear_session() # Memory clean-up\n",
        "\n",
        "num_epochs = 200\n",
        "all_mae_histories = []\n",
        "for i in range(k):\n",
        "    print('processing fold #', i)\n",
        "    # Prepare the validation data: data from partition # k\n",
        "    val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples]\n",
        "    val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples]\n",
        "\n",
        "    # Prepare the training data: data from all other partitions\n",
        "    partial_train_data = np.concatenate(\n",
        "        [train_data[:i * num_val_samples],\n",
        "         train_data[(i + 1) * num_val_samples:]],\n",
        "        axis=0)\n",
        "    partial_train_targets = np.concatenate(\n",
        "        [train_targets[:i * num_val_samples],\n",
        "         train_targets[(i + 1) * num_val_samples:]],\n",
        "        axis=0)\n",
        "\n",
        "    # Build the Keras model (already compiled)\n",
        "    model = build_model()\n",
        "    # Train the model (in silent mode, verbose=0)\n",
        "    history = model.fit(partial_train_data, partial_train_targets,\n",
        "                        validation_data=(val_data, val_targets),\n",
        "                        epochs=num_epochs, batch_size=1, verbose=2)\n",
        "    mae_history = history.history['val_loss']\n",
        "    all_mae_histories.append(mae_history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "g1EJEG6a9Ew-",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ARqVvyMX9ExF"
      },
      "source": [
        "#### Takeaways\n",
        "* Regression is usually done using MSE loss and MAE for evaluation\n",
        "* Input data should always be scaled (independent from the test set)\n",
        "* Small datasets:\n",
        "    - Use cross-validation\n",
        "    - Use simple (non-deep) networks\n",
        "    - Smaller batches, more epochs"
      ]
    }
  ]
}